{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-28 10:29:21,047] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-08-28 10:29:22,271] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/qwen_omni_finetuned_common_voice_for_qwen_train_less_than_3_sec/final_model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor loaded.\n",
      "Loading evaluation data from: /raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/data/common_voice_for_qwen/less_than_3_sec/processed/streaming_chunks_padded.jsonl\n",
      "\n",
      "Starting evaluation on 4793 audio chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Gate Predictions: 100%|██████████| 4793/4793 [10:34<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Overall Accuracy: 63.59% (3048 / 4793)\n",
      "\n",
      "Confusion Matrix (Positive = TRANSLATE):\n",
      "  TP:  516   TN: 2532   FP: 1390   FN:  355\n",
      "\n",
      "Metrics (TRANSLATE):\n",
      "  Precision: 0.2707\n",
      "  Recall:    0.5924\n",
      "  F1-Score:  0.3716\n",
      "\n",
      "Total runtime: 640.25 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    Qwen2_5OmniProcessor,\n",
    "    Qwen2_5OmniThinkerForConditionalGeneration,  # OK to keep; we’ll also handle fallback\n",
    ")\n",
    "\n",
    "# ==== CONFIG ====\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "MODEL_PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/qwen_omni_finetuned_common_voice_for_qwen_train_less_than_3_sec/final_model_1_epoch\"\n",
    "JSONL_PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/data/common_voice_for_qwen/less_than_3_sec/processed/streaming_chunks_padded.jsonl\"\n",
    "TRANSLATE_THRESHOLD = 0.1\n",
    "USE_AUTOCast = False  # set True if you want cuda autocast(bfloat16) during forward\n",
    "# =================\n",
    "\n",
    "\n",
    "def load_and_prep_audio(audio_path: str, target_sr: int) -> torch.Tensor:\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav.mean(0, keepdim=True)\n",
    "    if sr != target_sr:\n",
    "        wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
    "    return wav.squeeze(0)  # [S]\n",
    "\n",
    "\n",
    "def _pool_last_audio_token(hidden_states: torch.Tensor,\n",
    "                           input_ids: torch.LongTensor,\n",
    "                           attention_mask: torch.LongTensor,\n",
    "                           audio_token_index: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    hidden_states: [B, T, D] (last hidden layer from the text model)\n",
    "    input_ids:     [B, T]\n",
    "    attention_mask:[B, T]\n",
    "    Returns:       [B, D] pooled at last audio token; fallback = last attended token.\n",
    "    \"\"\"\n",
    "    B, T, D = hidden_states.shape\n",
    "    device = hidden_states.device\n",
    "\n",
    "    audio_mask = (input_ids == audio_token_index)  # [B, T]\n",
    "    has_audio = audio_mask.any(dim=1)              # [B]\n",
    "\n",
    "    # default: last attended token\n",
    "    # (find last index where attention_mask==1)\n",
    "    attn = attention_mask.to(torch.int8)\n",
    "    rev = torch.flip(attn, dims=[1])                        # [B, T]\n",
    "    last_from_end = torch.argmax(rev, dim=1)                # [B]\n",
    "    last_attn_idx = (T - 1) - last_from_end                 # [B]\n",
    "    last_idx = last_attn_idx.clone()\n",
    "\n",
    "    # where audio exists, take last audio position instead\n",
    "    if has_audio.any():\n",
    "        rev_audio = torch.flip(audio_mask.to(torch.int8), dims=[1])\n",
    "        last_audio_from_end = torch.argmax(rev_audio, dim=1)\n",
    "        last_audio_idx = (T - 1) - last_audio_from_end\n",
    "        last_idx = torch.where(has_audio, last_audio_idx, last_attn_idx)\n",
    "\n",
    "    gather_idx = last_idx.view(B, 1, 1).expand(B, 1, D)     # [B,1,D]\n",
    "    pooled = hidden_states.gather(dim=1, index=gather_idx).squeeze(1)\n",
    "    return pooled  # [B, D]\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_gate_prediction(model, processor, wav_tensor: torch.Tensor, system_prompt: dict) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns probabilities over [WAIT, TRANSLATE].\n",
    "    Works if the model natively returns gate_logits OR if we have to compute them\n",
    "    from hidden_states + model.conaiki_gate.\n",
    "    \"\"\"\n",
    "    target_sr = processor.feature_extractor.sampling_rate\n",
    "\n",
    "    # conversation (minimal, mirrors training structure on the user/audio side)\n",
    "    conversation = [\n",
    "        system_prompt,\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"audio\", \"audio_url\": \"placeholder.wav\"}]},\n",
    "    ]\n",
    "    text = processor.apply_chat_template(conversation, add_generation_prompt=False, tokenize=False)\n",
    "\n",
    "    # Prepare inputs (single example)\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        audio=[wav_tensor.cpu().numpy()],\n",
    "        sampling_rate=target_sr,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    # Move to the right device\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Try the fast path: model returns gate_logits directly (your patched forward).\n",
    "    try:\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=(model.device.type == \"cuda\" and USE_AUTOCast)):\n",
    "            outputs = model(**inputs, return_gate_logits=True)\n",
    "        if hasattr(outputs, \"gate_logits\") and outputs.gate_logits is not None:\n",
    "            probs = torch.softmax(outputs.gate_logits.float(), dim=-1).squeeze(0)\n",
    "            return probs\n",
    "    except TypeError:\n",
    "        # The model forward might not accept return_gate_logits; we’ll fall back below.\n",
    "        pass\n",
    "\n",
    "    # Fallback: compute gate_logits ourselves from hidden states.\n",
    "    if not hasattr(model, \"conaiki_gate\"):\n",
    "        raise RuntimeError(\n",
    "            \"Model does not expose 'gate_logits' AND has no 'conaiki_gate' module. \"\n",
    "            \"Load your custom class (with gate head) or re-export the model with that head.\"\n",
    "        )\n",
    "\n",
    "    # We need last hidden states BEFORE lm_head; ask the model for hidden_states.\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=(model.device.type == \"cuda\" and USE_AUTOCast)):\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    if not hasattr(outputs, \"hidden_states\") or outputs.hidden_states is None:\n",
    "        raise RuntimeError(\"Model did not return hidden_states; cannot build gate logits fallback.\")\n",
    "\n",
    "    # The last entry in hidden_states is the last layer of the text model (before lm_head).\n",
    "    # NOTE: For Qwen* decoders, hidden_states is a list of all layer outputs. Pick the last.\n",
    "    last_h = outputs.hidden_states[-1]  # [B, T, D]\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    audio_idx = getattr(model.config, \"audio_token_index\", None)\n",
    "    if audio_idx is None:\n",
    "        # If the config doesn’t have it, try a reasonable fallback.\n",
    "        # But ideally, this should exist in Qwen-Omni configs.\n",
    "        audio_idx = 151666  # (example) – replace with your actual audio token id if known.\n",
    "\n",
    "    pooled = _pool_last_audio_token(last_h, input_ids, attention_mask, audio_idx)  # [B, D]\n",
    "    gate_logits = model.conaiki_gate(pooled)  # [B, 2] for WAIT/TRANSLATE (or [B, C] if you set C=2)\n",
    "    probs = torch.softmax(gate_logits.float(), dim=-1).squeeze(0)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def evaluate_gate_model():\n",
    "    start = time.perf_counter()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(f\"Loading model from: {MODEL_PATH} ...\")\n",
    "    model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "        trust_remote_code=True,   # important if your class was saved with custom code\n",
    "    ).to(device).eval()\n",
    "    processor = Qwen2_5OmniProcessor.from_pretrained(MODEL_PATH)\n",
    "    print(\"Model and processor loaded.\")\n",
    "\n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}],\n",
    "    }\n",
    "\n",
    "    # Load dataset\n",
    "    print(f\"Loading evaluation data from: {JSONL_PATH}\")\n",
    "    jsonl_path = Path(JSONL_PATH).expanduser().resolve()\n",
    "    base_dir = jsonl_path.parent\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        samples = [json.loads(line) for line in f]\n",
    "\n",
    "    WAIT_IDX, TRANS_IDX = 0, 1\n",
    "    label_map = {\"WAIT\": WAIT_IDX, \"TRANSLATE\": TRANS_IDX}\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    cm = {\"tp\": 0, \"tn\": 0, \"fp\": 0, \"fn\": 0}\n",
    "\n",
    "    print(f\"\\nStarting evaluation on {len(samples)} audio chunks...\")\n",
    "    pbar = tqdm(samples, desc=\"Evaluating Gate Predictions\")\n",
    "    for sample in pbar:\n",
    "        try:\n",
    "            true_lbl_s = sample[\"gate_label\"]\n",
    "            true_lbl = label_map[true_lbl_s]\n",
    "\n",
    "            audio_rel = sample[\"audio_path\"]\n",
    "            audio_path = (base_dir / audio_rel).as_posix()\n",
    "\n",
    "            wav = load_and_prep_audio(audio_path, processor.feature_extractor.sampling_rate)\n",
    "            probs = get_gate_prediction(model, processor, wav, system_prompt)\n",
    "\n",
    "            p_translate = probs[TRANS_IDX].item()\n",
    "            pred_lbl = TRANS_IDX if p_translate >= TRANSLATE_THRESHOLD else WAIT_IDX\n",
    "\n",
    "            correct += int(pred_lbl == true_lbl)\n",
    "            if   pred_lbl == TRANS_IDX and true_lbl == TRANS_IDX: cm[\"tp\"] += 1\n",
    "            elif pred_lbl == WAIT_IDX  and true_lbl == WAIT_IDX:  cm[\"tn\"] += 1\n",
    "            elif pred_lbl == TRANS_IDX and true_lbl == WAIT_IDX:  cm[\"fp\"] += 1\n",
    "            elif pred_lbl == WAIT_IDX  and true_lbl == TRANS_IDX: cm[\"fn\"] += 1\n",
    "\n",
    "            total += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nSkipping sample due to error: {e} | Sample: {sample.get('audio_path')}\")\n",
    "\n",
    "    # Report\n",
    "    print(\"\\n--- Evaluation Complete ---\")\n",
    "    acc = (correct / total * 100.0) if total else 0.0\n",
    "    print(f\"Overall Accuracy: {acc:.2f}% ({correct} / {total})\")\n",
    "\n",
    "    tp, tn, fp, fn = cm[\"tp\"], cm[\"tn\"], cm[\"fp\"], cm[\"fn\"]\n",
    "    print(\"\\nConfusion Matrix (Positive = TRANSLATE):\")\n",
    "    print(f\"  TP: {tp:4d}   TN: {tn:4d}   FP: {fp:4d}   FN: {fn:4d}\")\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "    print(\"\\nMetrics (TRANSLATE):\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "    print(f\"\\nTotal runtime: {time.perf_counter() - start:.2f} s\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_gate_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-28 12:10:11,567] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-08-28 12:10:13,061] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/qwen_omni_finetuned_common_voice_for_qwen_train_less_than_3_sec/final_model_1_epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor loaded.\n",
      "Loading evaluation data from: /raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/data/common_voice_for_qwen/less_than_3_sec/processed/streaming_chunks_padded.jsonl\n",
      "Loaded 4793 samples for evaluation.\n",
      "\n",
      "Generating predictions for all samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing predictions: 100%|██████████| 4793/4793 [10:01<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating 90 different thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing thresholds: 100%|██████████| 90/90 [00:00<00:00, 803.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE\n",
      "============================================================\n",
      "Results saved to: /raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/evaluation/gate_threshold_evaluation_results.json\n",
      "\n",
      "Best threshold (by F1): 0.10\n",
      "  - F1 Score: 0.3953\n",
      "  - Accuracy: 0.5838\n",
      "  - Precision: 0.2685\n",
      "  - Recall: 0.7486\n",
      "\n",
      "Total runtime: 607.37 seconds\n",
      "\n",
      "============================================================\n",
      "THRESHOLD COMPARISON TABLE\n",
      "============================================================\n",
      "Threshold  Accuracy   F1         Precision  Recall    \n",
      "------------------------------------------------------------\n",
      "0.10       0.5838     0.3953     0.2685     0.7486    \n",
      "0.11       0.5884     0.3950     0.2695     0.7394    \n",
      "0.12       0.5904     0.3928     0.2688     0.7290    \n",
      "0.13       0.5927     0.3911     0.2685     0.7199    \n",
      "0.14       0.5944     0.3883     0.2674     0.7084    \n",
      "0.15       0.6003     0.3875     0.2685     0.6958    \n",
      "0.16       0.6005     0.3836     0.2665     0.6843    \n",
      "0.17       0.6038     0.3828     0.2670     0.6762    \n",
      "0.18       0.6082     0.3847     0.2691     0.6739    \n",
      "0.19       0.6090     0.3799     0.2669     0.6590    \n",
      "0.20       0.6117     0.3786     0.2669     0.6510    \n",
      "0.21       0.6157     0.3777     0.2676     0.6418    \n",
      "0.22       0.6174     0.3775     0.2680     0.6383    \n",
      "0.23       0.6209     0.3771     0.2688     0.6315    \n",
      "0.24       0.6234     0.3752     0.2686     0.6223    \n",
      "0.25       0.6265     0.3741     0.2690     0.6142    \n",
      "0.26       0.6299     0.3700     0.2679     0.5982    \n",
      "0.27       0.6334     0.3700     0.2690     0.5924    \n",
      "0.28       0.6380     0.3716     0.2714     0.5890    \n",
      "0.29       0.6434     0.3738     0.2745     0.5855    \n",
      "0.30       0.6472     0.3735     0.2757     0.5786    \n",
      "0.31       0.6501     0.3726     0.2764     0.5718    \n",
      "0.32       0.6518     0.3709     0.2761     0.5649    \n",
      "0.33       0.6545     0.3703     0.2769     0.5591    \n",
      "0.34       0.6568     0.3690     0.2771     0.5522    \n",
      "0.35       0.6618     0.3700     0.2797     0.5465    \n",
      "0.36       0.6658     0.3703     0.2815     0.5408    \n",
      "0.37       0.6678     0.3678     0.2811     0.5316    \n",
      "0.38       0.6699     0.3636     0.2799     0.5189    \n",
      "0.39       0.6729     0.3626     0.2807     0.5121    \n",
      "0.40       0.6766     0.3627     0.2825     0.5063    \n",
      "0.41       0.6787     0.3610     0.2827     0.4994    \n",
      "0.42       0.6789     0.3579     0.2811     0.4925    \n",
      "0.43       0.6793     0.3556     0.2801     0.4868    \n",
      "0.44       0.6837     0.3554     0.2822     0.4799    \n",
      "0.45       0.6860     0.3538     0.2826     0.4730    \n",
      "0.46       0.6887     0.3530     0.2836     0.4673    \n",
      "0.47       0.6939     0.3535     0.2868     0.4604    \n",
      "0.48       0.6950     0.3520     0.2866     0.4558    \n",
      "0.49       0.6966     0.3509     0.2871     0.4512    \n",
      "0.50       0.6977     0.3482     0.2862     0.4443    \n",
      "0.51       0.7019     0.3484     0.2890     0.4386    \n",
      "0.52       0.7048     0.3482     0.2908     0.4340    \n",
      "0.53       0.7064     0.3477     0.2916     0.4305    \n",
      "0.54       0.7092     0.3480     0.2936     0.4271    \n",
      "0.55       0.7106     0.3442     0.2926     0.4179    \n",
      "0.56       0.7123     0.3436     0.2935     0.4145    \n",
      "0.57       0.7140     0.3424     0.2941     0.4099    \n",
      "0.58       0.7165     0.3432     0.2963     0.4076    \n",
      "0.59       0.7183     0.3402     0.2962     0.3995    \n",
      "0.60       0.7192     0.3376     0.2954     0.3938    \n",
      "0.61       0.7204     0.3327     0.2938     0.3835    \n",
      "0.62       0.7236     0.3352     0.2977     0.3835    \n",
      "0.63       0.7238     0.3286     0.2943     0.3720    \n",
      "0.64       0.7267     0.3296     0.2973     0.3697    \n",
      "0.65       0.7286     0.3276     0.2979     0.3639    \n",
      "0.66       0.7306     0.3244     0.2981     0.3559    \n",
      "0.67       0.7327     0.3219     0.2986     0.3490    \n",
      "0.68       0.7342     0.3180     0.2979     0.3410    \n",
      "0.69       0.7346     0.3139     0.2960     0.3341    \n",
      "0.70       0.7357     0.3103     0.2950     0.3272    \n",
      "0.71       0.7365     0.3079     0.2945     0.3226    \n",
      "0.72       0.7377     0.3044     0.2938     0.3157    \n",
      "0.73       0.7392     0.3025     0.2942     0.3111    \n",
      "0.74       0.7417     0.3021     0.2968     0.3077    \n",
      "0.75       0.7434     0.3019     0.2985     0.3054    \n",
      "0.76       0.7461     0.2986     0.2998     0.2974    \n",
      "0.77       0.7496     0.2999     0.3049     0.2951    \n",
      "0.78       0.7501     0.2936     0.3018     0.2859    \n",
      "0.79       0.7505     0.2906     0.3006     0.2813    \n",
      "0.80       0.7515     0.2838     0.2980     0.2710    \n",
      "0.81       0.7540     0.2815     0.3000     0.2652    \n",
      "0.82       0.7542     0.2764     0.2972     0.2583    \n",
      "0.83       0.7555     0.2739     0.2974     0.2537    \n",
      "0.84       0.7569     0.2678     0.2958     0.2445    \n",
      "0.85       0.7586     0.2635     0.2957     0.2377    \n",
      "0.86       0.7615     0.2602     0.2982     0.2308    \n",
      "0.87       0.7638     0.2572     0.3002     0.2250    \n",
      "0.88       0.7674     0.2572     0.3063     0.2216    \n",
      "0.89       0.7690     0.2565     0.3091     0.2193    \n",
      "0.90       0.7732     0.2570     0.3176     0.2158    \n",
      "0.91       0.7749     0.2502     0.3169     0.2067    \n",
      "0.92       0.7772     0.2426     0.3173     0.1963    \n",
      "0.93       0.7778     0.2333     0.3127     0.1860    \n",
      "0.94       0.7809     0.2279     0.3170     0.1780    \n",
      "0.95       0.7834     0.2064     0.3089     0.1550    \n",
      "0.96       0.7845     0.1936     0.3024     0.1424    \n",
      "0.97       0.7878     0.1871     0.3079     0.1343    \n",
      "0.98       0.7966     0.1744     0.3323     0.1183    \n",
      "0.99       0.8014     0.1454     0.3333     0.0930    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    Qwen2_5OmniProcessor,\n",
    "    Qwen2_5OmniThinkerForConditionalGeneration,\n",
    ")\n",
    "\n",
    "# ==== CONFIG ====\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "MODEL_PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/qwen_omni_finetuned_common_voice_for_qwen_train_less_than_3_sec/final_model_1_epoch\"\n",
    "JSONL_PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/data/common_voice_for_qwen/less_than_3_sec/processed/streaming_chunks_padded.jsonl\"\n",
    "\n",
    "# Threshold range configuration\n",
    "THRESHOLD_START = 0.1\n",
    "THRESHOLD_END = 0.99\n",
    "THRESHOLD_STEP = 0.01\n",
    "\n",
    "# Output file for results\n",
    "OUTPUT_JSON_PATH = \"gate_threshold_evaluation_results.json\"\n",
    "\n",
    "USE_AUTOCAST = False  # set True if you want cuda autocast(bfloat16) during forward\n",
    "# =================\n",
    "\n",
    "\n",
    "def load_and_prep_audio(audio_path: str, target_sr: int) -> torch.Tensor:\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav.mean(0, keepdim=True)\n",
    "    if sr != target_sr:\n",
    "        wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
    "    return wav.squeeze(0)  # [S]\n",
    "\n",
    "\n",
    "def _pool_last_audio_token(hidden_states: torch.Tensor,\n",
    "                           input_ids: torch.LongTensor,\n",
    "                           attention_mask: torch.LongTensor,\n",
    "                           audio_token_index: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    hidden_states: [B, T, D] (last hidden layer from the text model)\n",
    "    input_ids:     [B, T]\n",
    "    attention_mask:[B, T]\n",
    "    Returns:       [B, D] pooled at last audio token; fallback = last attended token.\n",
    "    \"\"\"\n",
    "    B, T, D = hidden_states.shape\n",
    "    device = hidden_states.device\n",
    "\n",
    "    audio_mask = (input_ids == audio_token_index)  # [B, T]\n",
    "    has_audio = audio_mask.any(dim=1)              # [B]\n",
    "\n",
    "    # default: last attended token\n",
    "    # (find last index where attention_mask==1)\n",
    "    attn = attention_mask.to(torch.int8)\n",
    "    rev = torch.flip(attn, dims=[1])                        # [B, T]\n",
    "    last_from_end = torch.argmax(rev, dim=1)                # [B]\n",
    "    last_attn_idx = (T - 1) - last_from_end                 # [B]\n",
    "    last_idx = last_attn_idx.clone()\n",
    "\n",
    "    # where audio exists, take last audio position instead\n",
    "    if has_audio.any():\n",
    "        rev_audio = torch.flip(audio_mask.to(torch.int8), dims=[1])\n",
    "        last_audio_from_end = torch.argmax(rev_audio, dim=1)\n",
    "        last_audio_idx = (T - 1) - last_audio_from_end\n",
    "        last_idx = torch.where(has_audio, last_audio_idx, last_attn_idx)\n",
    "\n",
    "    gather_idx = last_idx.view(B, 1, 1).expand(B, 1, D)     # [B,1,D]\n",
    "    pooled = hidden_states.gather(dim=1, index=gather_idx).squeeze(1)\n",
    "    return pooled  # [B, D]\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_gate_prediction(model, processor, wav_tensor: torch.Tensor, system_prompt: dict) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns probabilities over [WAIT, TRANSLATE].\n",
    "    Works if the model natively returns gate_logits OR if we have to compute them\n",
    "    from hidden_states + model.conaiki_gate.\n",
    "    \"\"\"\n",
    "    target_sr = processor.feature_extractor.sampling_rate\n",
    "\n",
    "    # conversation (minimal, mirrors training structure on the user/audio side)\n",
    "    conversation = [\n",
    "        system_prompt,\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"audio\", \"audio_url\": \"placeholder.wav\"}]},\n",
    "    ]\n",
    "    text = processor.apply_chat_template(conversation, add_generation_prompt=False, tokenize=False)\n",
    "\n",
    "    # Prepare inputs (single example)\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        audio=[wav_tensor.cpu().numpy()],\n",
    "        sampling_rate=target_sr,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    # Move to the right device\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Try the fast path: model returns gate_logits directly (your patched forward).\n",
    "    try:\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=(model.device.type == \"cuda\" and USE_AUTOCAST)):\n",
    "            outputs = model(**inputs, return_gate_logits=True)\n",
    "        if hasattr(outputs, \"gate_logits\") and outputs.gate_logits is not None:\n",
    "            probs = torch.softmax(outputs.gate_logits.float(), dim=-1).squeeze(0)\n",
    "            return probs\n",
    "    except TypeError:\n",
    "        # The model forward might not accept return_gate_logits; we'll fall back below.\n",
    "        pass\n",
    "\n",
    "    # Fallback: compute gate_logits ourselves from hidden states.\n",
    "    if not hasattr(model, \"conaiki_gate\"):\n",
    "        raise RuntimeError(\n",
    "            \"Model does not expose 'gate_logits' AND has no 'conaiki_gate' module. \"\n",
    "            \"Load your custom class (with gate head) or re-export the model with that head.\"\n",
    "        )\n",
    "\n",
    "    # We need last hidden states BEFORE lm_head; ask the model for hidden_states.\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=(model.device.type == \"cuda\" and USE_AUTOCAST)):\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    if not hasattr(outputs, \"hidden_states\") or outputs.hidden_states is None:\n",
    "        raise RuntimeError(\"Model did not return hidden_states; cannot build gate logits fallback.\")\n",
    "\n",
    "    # The last entry in hidden_states is the last layer of the text model (before lm_head).\n",
    "    # NOTE: For Qwen* decoders, hidden_states is a list of all layer outputs. Pick the last.\n",
    "    last_h = outputs.hidden_states[-1]  # [B, T, D]\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    audio_idx = getattr(model.config, \"audio_token_index\", None)\n",
    "    if audio_idx is None:\n",
    "        # If the config doesn't have it, try a reasonable fallback.\n",
    "        # But ideally, this should exist in Qwen-Omni configs.\n",
    "        audio_idx = 151666  # (example) – replace with your actual audio token id if known.\n",
    "\n",
    "    pooled = _pool_last_audio_token(last_h, input_ids, attention_mask, audio_idx)  # [B, D]\n",
    "    gate_logits = model.conaiki_gate(pooled)  # [B, 2] for WAIT/TRANSLATE (or [B, C] if you set C=2)\n",
    "    probs = torch.softmax(gate_logits.float(), dim=-1).squeeze(0)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def calculate_metrics(tp, tn, fp, fn):\n",
    "    \"\"\"Calculate precision, recall, F1, and other metrics.\"\"\"\n",
    "    accuracy = ((tp + tn) / (tp + tn + fp + fn)) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    \n",
    "    # Matthews Correlation Coefficient\n",
    "    mcc_num = (tp * tn - fp * fn)\n",
    "    mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    mcc = mcc_num / mcc_den if mcc_den > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"specificity\": specificity,\n",
    "        \"mcc\": mcc\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_single_threshold(samples, model, processor, system_prompt, threshold, base_dir):\n",
    "    \"\"\"Evaluate model performance at a single threshold.\"\"\"\n",
    "    WAIT_IDX, TRANS_IDX = 0, 1\n",
    "    label_map = {\"WAIT\": WAIT_IDX, \"TRANSLATE\": TRANS_IDX}\n",
    "    \n",
    "    tp = tn = fp = fn = 0\n",
    "    predictions = []\n",
    "    errors = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        try:\n",
    "            true_lbl_s = sample[\"gate_label\"]\n",
    "            true_lbl = label_map[true_lbl_s]\n",
    "            \n",
    "            audio_rel = sample[\"audio_path\"]\n",
    "            audio_path = (base_dir / audio_rel).as_posix()\n",
    "            \n",
    "            wav = load_and_prep_audio(audio_path, processor.feature_extractor.sampling_rate)\n",
    "            probs = get_gate_prediction(model, processor, wav, system_prompt)\n",
    "            \n",
    "            p_wait = probs[WAIT_IDX].item()\n",
    "            p_translate = probs[TRANS_IDX].item()\n",
    "            pred_lbl = TRANS_IDX if p_translate >= threshold else WAIT_IDX\n",
    "            \n",
    "            # Update confusion matrix\n",
    "            if pred_lbl == TRANS_IDX and true_lbl == TRANS_IDX:\n",
    "                tp += 1\n",
    "            elif pred_lbl == WAIT_IDX and true_lbl == WAIT_IDX:\n",
    "                tn += 1\n",
    "            elif pred_lbl == TRANS_IDX and true_lbl == WAIT_IDX:\n",
    "                fp += 1\n",
    "            elif pred_lbl == WAIT_IDX and true_lbl == TRANS_IDX:\n",
    "                fn += 1\n",
    "            \n",
    "            predictions.append({\n",
    "                \"audio_path\": audio_rel,\n",
    "                \"true_label\": true_lbl_s,\n",
    "                \"predicted_label\": \"TRANSLATE\" if pred_lbl == TRANS_IDX else \"WAIT\",\n",
    "                \"p_wait\": p_wait,\n",
    "                \"p_translate\": p_translate,\n",
    "                \"correct\": pred_lbl == true_lbl\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append({\n",
    "                \"audio_path\": sample.get('audio_path', 'unknown'),\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    metrics = calculate_metrics(tp, tn, fp, fn)\n",
    "    \n",
    "    return {\n",
    "        \"threshold\": threshold,\n",
    "        \"confusion_matrix\": {\n",
    "            \"tp\": tp,\n",
    "            \"tn\": tn,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn\n",
    "        },\n",
    "        \"metrics\": metrics,\n",
    "        \"total_samples\": len(samples),\n",
    "        \"processed_samples\": len(predictions),\n",
    "        \"error_count\": len(errors),\n",
    "        \"errors\": errors if errors else None\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_gate_model_multi_threshold():\n",
    "    \"\"\"Main evaluation function testing multiple thresholds.\"\"\"\n",
    "    overall_start = time.perf_counter()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"Loading model from: {MODEL_PATH} ...\")\n",
    "    model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "    ).to(device).eval()\n",
    "    processor = Qwen2_5OmniProcessor.from_pretrained(MODEL_PATH)\n",
    "    print(\"Model and processor loaded.\")\n",
    "    \n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}],\n",
    "    }\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"Loading evaluation data from: {JSONL_PATH}\")\n",
    "    jsonl_path = Path(JSONL_PATH).expanduser().resolve()\n",
    "    base_dir = jsonl_path.parent\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        samples = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"Loaded {len(samples)} samples for evaluation.\")\n",
    "    \n",
    "    # Generate thresholds to test\n",
    "    thresholds = np.arange(THRESHOLD_START, THRESHOLD_END + THRESHOLD_STEP, THRESHOLD_STEP)\n",
    "    thresholds = np.round(thresholds, 2)  # Avoid floating point precision issues\n",
    "    \n",
    "    # Store results\n",
    "    all_results = {\n",
    "        \"model_path\": MODEL_PATH,\n",
    "        \"dataset_path\": JSONL_PATH,\n",
    "        \"evaluation_timestamp\": datetime.now().isoformat(),\n",
    "        \"total_samples\": len(samples),\n",
    "        \"threshold_results\": [],\n",
    "        \"best_threshold\": None,\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    # First pass: get predictions for all samples (cache them)\n",
    "    print(\"\\nGenerating predictions for all samples...\")\n",
    "    WAIT_IDX, TRANS_IDX = 0, 1\n",
    "    label_map = {\"WAIT\": WAIT_IDX, \"TRANSLATE\": TRANS_IDX}\n",
    "    \n",
    "    cached_predictions = []\n",
    "    pbar = tqdm(samples, desc=\"Computing predictions\")\n",
    "    \n",
    "    for sample in pbar:\n",
    "        try:\n",
    "            true_lbl_s = sample[\"gate_label\"]\n",
    "            true_lbl = label_map[true_lbl_s]\n",
    "            \n",
    "            audio_rel = sample[\"audio_path\"]\n",
    "            audio_path = (base_dir / audio_rel).as_posix()\n",
    "            \n",
    "            wav = load_and_prep_audio(audio_path, processor.feature_extractor.sampling_rate)\n",
    "            probs = get_gate_prediction(model, processor, wav, system_prompt)\n",
    "            \n",
    "            cached_predictions.append({\n",
    "                \"audio_path\": audio_rel,\n",
    "                \"true_label\": true_lbl_s,\n",
    "                \"true_label_idx\": true_lbl,\n",
    "                \"p_wait\": probs[WAIT_IDX].item(),\n",
    "                \"p_translate\": probs[TRANS_IDX].item()\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {sample.get('audio_path')}: {e}\")\n",
    "            cached_predictions.append({\n",
    "                \"audio_path\": sample.get('audio_path', 'unknown'),\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    # Evaluate each threshold using cached predictions\n",
    "    print(f\"\\nEvaluating {len(thresholds)} different thresholds...\")\n",
    "    best_f1 = -1\n",
    "    best_threshold_info = None\n",
    "    \n",
    "    for threshold in tqdm(thresholds, desc=\"Testing thresholds\"):\n",
    "        tp = tn = fp = fn = 0\n",
    "        \n",
    "        for pred in cached_predictions:\n",
    "            if \"error\" in pred:\n",
    "                continue\n",
    "                \n",
    "            true_lbl = pred[\"true_label_idx\"]\n",
    "            p_translate = pred[\"p_translate\"]\n",
    "            pred_lbl = TRANS_IDX if p_translate >= threshold else WAIT_IDX\n",
    "            \n",
    "            if pred_lbl == TRANS_IDX and true_lbl == TRANS_IDX:\n",
    "                tp += 1\n",
    "            elif pred_lbl == WAIT_IDX and true_lbl == WAIT_IDX:\n",
    "                tn += 1\n",
    "            elif pred_lbl == TRANS_IDX and true_lbl == WAIT_IDX:\n",
    "                fp += 1\n",
    "            elif pred_lbl == WAIT_IDX and true_lbl == TRANS_IDX:\n",
    "                fn += 1\n",
    "        \n",
    "        metrics = calculate_metrics(tp, tn, fp, fn)\n",
    "        \n",
    "        threshold_result = {\n",
    "            \"threshold\": float(threshold),\n",
    "            \"confusion_matrix\": {\n",
    "                \"tp\": tp,\n",
    "                \"tn\": tn,\n",
    "                \"fp\": fp,\n",
    "                \"fn\": fn\n",
    "            },\n",
    "            \"metrics\": {k: float(v) for k, v in metrics.items()},\n",
    "            \"processed_samples\": sum([1 for p in cached_predictions if \"error\" not in p])\n",
    "        }\n",
    "        \n",
    "        all_results[\"threshold_results\"].append(threshold_result)\n",
    "        \n",
    "        # Track best threshold by F1 score\n",
    "        if metrics[\"f1_score\"] > best_f1:\n",
    "            best_f1 = metrics[\"f1_score\"]\n",
    "            best_threshold_info = threshold_result\n",
    "    \n",
    "    # Set best threshold\n",
    "    all_results[\"best_threshold\"] = best_threshold_info\n",
    "    \n",
    "    # Create summary statistics\n",
    "    all_accuracies = [r[\"metrics\"][\"accuracy\"] for r in all_results[\"threshold_results\"]]\n",
    "    all_f1_scores = [r[\"metrics\"][\"f1_score\"] for r in all_results[\"threshold_results\"]]\n",
    "    all_precisions = [r[\"metrics\"][\"precision\"] for r in all_results[\"threshold_results\"]]\n",
    "    all_recalls = [r[\"metrics\"][\"recall\"] for r in all_results[\"threshold_results\"]]\n",
    "    \n",
    "    all_results[\"summary\"] = {\n",
    "        \"accuracy_range\": [float(min(all_accuracies)), float(max(all_accuracies))],\n",
    "        \"f1_range\": [float(min(all_f1_scores)), float(max(all_f1_scores))],\n",
    "        \"precision_range\": [float(min(all_precisions)), float(max(all_precisions))],\n",
    "        \"recall_range\": [float(min(all_recalls)), float(max(all_recalls))],\n",
    "        \"best_f1_threshold\": float(best_threshold_info[\"threshold\"]),\n",
    "        \"best_f1_score\": float(best_f1),\n",
    "        \"total_runtime_seconds\": float(time.perf_counter() - overall_start)\n",
    "    }\n",
    "    \n",
    "    # Save results to JSON\n",
    "    output_path = Path(OUTPUT_JSON_PATH)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Results saved to: {output_path.absolute()}\")\n",
    "    print(f\"\\nBest threshold (by F1): {best_threshold_info['threshold']:.2f}\")\n",
    "    print(f\"  - F1 Score: {best_threshold_info['metrics']['f1_score']:.4f}\")\n",
    "    print(f\"  - Accuracy: {best_threshold_info['metrics']['accuracy']:.4f}\")\n",
    "    print(f\"  - Precision: {best_threshold_info['metrics']['precision']:.4f}\")\n",
    "    print(f\"  - Recall: {best_threshold_info['metrics']['recall']:.4f}\")\n",
    "    print(f\"\\nTotal runtime: {time.perf_counter() - overall_start:.2f} seconds\")\n",
    "    \n",
    "    # Print a quick summary table\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"THRESHOLD COMPARISON TABLE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Threshold':<10} {'Accuracy':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for result in all_results[\"threshold_results\"]:\n",
    "        t = result[\"threshold\"]\n",
    "        m = result[\"metrics\"]\n",
    "        print(f\"{t:<10.2f} {m['accuracy']:<10.4f} {m['f1_score']:<10.4f} {m['precision']:<10.4f} {m['recall']:<10.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_gate_model_multi_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5 \n",
    "\n",
    "--- Evaluation Complete ---\n",
    "Overall Accuracy: 54.83% (2628 / 4793)\n",
    "\n",
    "Confusion Matrix (Positive = TRANSLATE):\n",
    "  TP:  428   TN: 2200   FP: 1722   FN:  443\n",
    "\n",
    "Metrics (TRANSLATE):\n",
    "  Precision: 0.1991\n",
    "  Recall:    0.4914\n",
    "  F1-Score:  0.2833\n",
    "\n",
    "Total runtime: 616.64 s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conaiki_qwen_omni",
   "language": "python",
   "name": "conaiki_qwen_omni"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
