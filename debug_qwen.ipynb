{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, torch, math, torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "from transformers import Qwen2_5OmniProcessor\n",
    "from transformers import Qwen2_5OmniThinkerForConditionalGeneration\n",
    "\n",
    "PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/models/rezised_Qwen2.5-Omni-7B\"\n",
    "\n",
    "# Load model\n",
    "model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "    PATH, torch_dtype=torch.bfloat16, device_map=None\n",
    ").to(\"cuda\")\n",
    "\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(PATH)\n",
    "\n",
    "# First test: standard generation\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"audio\", \"path\": \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/rustem_1.wav\"}]},\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    [conversation],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(model.device)\n",
    "\n",
    "text_ids = model.generate(**inputs)\n",
    "text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(\"Standard generation:\", text[0])\n",
    "\n",
    "model.conaiki_align_modules()\n",
    "\n",
    "id_TRANSLATE = tok.convert_tokens_to_ids(\"<TRANSLATE>\")\n",
    "id_WAIT      = tok.convert_tokens_to_ids(\"<WAIT>\")\n",
    "id_SILENCE   = tok.convert_tokens_to_ids(\"<SILENCE>\")\n",
    "\n",
    "# 2) Prompt\n",
    "prompt_ids = tok.encode(\"<SRC=RU> <TGT=EN> <LAG=600ms>\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 3) Helpers\n",
    "def prune_kv_cache(past_kv, keep_last_n_positions):\n",
    "    if past_kv is None or keep_last_n_positions <= 0:\n",
    "        return past_kv\n",
    "    pruned = []\n",
    "    for (k, v) in past_kv:\n",
    "        k_pruned = k[:, :, -keep_last_n_positions:, :]\n",
    "        v_pruned = v[:, :, -keep_last_n_positions:, :]\n",
    "        pruned.append((k_pruned, v_pruned))\n",
    "    return tuple(pruned)\n",
    "\n",
    "def decide_action_from_logits(logits, action_ids, step_idx, state, patience_k=3):\n",
    "    # logits: [B, T_total, V]; we read last position\n",
    "    last = logits[:, -1, :]\n",
    "    action_logits = torch.stack([\n",
    "        last[..., action_ids['SILENCE']],\n",
    "        last[..., action_ids['WAIT']],\n",
    "        last[..., action_ids['TRANSLATE']]\n",
    "    ], dim=-1)                     # [B, 3]\n",
    "    p_sil, p_wait, p_tr = action_logits.softmax(-1)[0].tolist()\n",
    "\n",
    "    # patience (collect a few ticks before speaking)\n",
    "    if step_idx < patience_k:\n",
    "        return id_WAIT, \"WAITING\", f\"[Patience {step_idx}/{patience_k}]\"\n",
    "\n",
    "    # hysteresis thresholds (tune later)\n",
    "    tau_on  = 0.55   # start speaking\n",
    "    tau_off = 0.35   # stop speaking\n",
    "    tau_sil = 0.65   # strong silence\n",
    "\n",
    "    if p_sil > tau_sil:\n",
    "        return id_SILENCE, \"SILENCE\", f\"[SILENCE p={p_sil:.2f}]\"\n",
    "\n",
    "    if state == \"WAITING\":\n",
    "        if p_tr > tau_on:\n",
    "            return id_TRANSLATE, \"TRANSLATING\", f\"[START TRANSLATE p={p_tr:.2f}]\"\n",
    "        return id_WAIT, \"WAITING\", f\"[WAIT p={p_wait:.2f}]\"\n",
    "\n",
    "    if state == \"TRANSLATING\":\n",
    "        if p_tr < tau_off:\n",
    "            return id_WAIT, \"WAITING\", f\"[STOP TRANSLATE p_wait={p_wait:.2f}]\"\n",
    "        return id_TRANSLATE, \"TRANSLATING\", f\"[CONTINUE TRANSLATE p={p_tr:.2f}]\"\n",
    "\n",
    "    # state == \"SILENCE\"\n",
    "    return (id_WAIT, \"WAITING\", \"[RESUME]\")\n",
    "    \n",
    "def mel_chunk_to_external(audio_np, sr, t0, dur_s):\n",
    "    \"\"\"\n",
    "    Slice raw audio -> mel frames for [t0, t0+dur_s).\n",
    "    Returns:\n",
    "      external_audio_embeds: [1, T_mel, n_mels]  (float, on current device)\n",
    "      external_audio_times : [T_mel] (float32 seconds)\n",
    "    Notes:\n",
    "      - For Qwen-Omni the feature extractor is WhisperFeatureExtractor; its __call__\n",
    "        expects `raw_speech` (positional or keyword), not `audio=`.\n",
    "      - We pass a LIST [chunk] to make a batch of 1.\n",
    "    \"\"\"\n",
    "    start = int(t0 * sr)\n",
    "    end   = start + int(dur_s * sr)\n",
    "    chunk = audio_np[start:end]\n",
    "    need = int(dur_s * sr) - len(chunk)\n",
    "    if need > 0:\n",
    "        chunk = np.pad(chunk, (0, need))\n",
    "    # ensure float32\n",
    "    chunk = chunk.astype(np.float32, copy=False)\n",
    "\n",
    "    # ✅ IMPORTANT: use `raw_speech=` (or positional) and pass a list\n",
    "    fe = processor.feature_extractor(\n",
    "        raw_speech=[chunk],\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "    )\n",
    "    # `input_features`: [B=1, n_mels, T]\n",
    "    mels = fe[\"input_features\"].to(device)  # keep fp32; model will cast as needed\n",
    "    B, n_mels, T = mels.shape\n",
    "\n",
    "    # Your _prep_external_audio expects [B, T, 128] in the \"mel path\".\n",
    "    # If n_mels != 128 on your build, you have two options:\n",
    "    #   (a) accept it and handle inside _prep_external_audio, or\n",
    "    #   (b) project/pad here. We'll accept as-is and just transpose.\n",
    "    external_audio_embeds = mels.transpose(1, 2)  # [1, T, n_mels]\n",
    "\n",
    "    # Approximate per-frame times across the chunk\n",
    "    times = torch.linspace(t0, t0 + dur_s, T, device=device, dtype=torch.float32)\n",
    "\n",
    "    return external_audio_embeds, times\n",
    "\n",
    "\n",
    "# 4) Streaming params\n",
    "chunk_duration = 0.24  # seconds (stride)\n",
    "sr = processor.feature_extractor.sampling_rate\n",
    "\n",
    "# Load audio once for streaming sim\n",
    "wav_path = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/rustem_1.wav\"\n",
    "audio_np, _ = librosa.load(wav_path, sr=sr)\n",
    "total_dur = len(audio_np) / sr\n",
    "max_steps = int(np.ceil(total_dur / chunk_duration))\n",
    "\n",
    "# 5) Streaming loop\n",
    "past_kv = None\n",
    "text_ids = prompt_ids.clone()\n",
    "generated_tokens = []\n",
    "state = \"WAITING\"\n",
    "audio_positions_est = 0  # rough “token” positions for pruning budget\n",
    "\n",
    "print(\"\\n=== Streaming S2TT (mel external path) ===\")\n",
    "print(f\"Audio: {wav_path}  duration={total_dur:.2f}s  stride={chunk_duration:.2f}s\\n\")\n",
    "\n",
    "action_ids = {'TRANSLATE': id_TRANSLATE, 'WAIT': id_WAIT, 'SILENCE': id_SILENCE}\n",
    "\n",
    "for step in range(max_steps):\n",
    "    t0 = step * chunk_duration\n",
    "    print(f\"\\n--- Step {step} @ {t0:.2f}s ---\")\n",
    "\n",
    "    # (A) Build mel external features for this chunk\n",
    "    ext_emb, times_step = mel_chunk_to_external(audio_np, sr, t0, chunk_duration)\n",
    "\n",
    "    # (B) PROBE: feed audio + a dummy token to read logits (no cache update)\n",
    "    with torch.no_grad():\n",
    "        dummy_id = tok.pad_token_id if tok.pad_token_id is not None else 0\n",
    "        dummy_token = torch.tensor([[dummy_id]], device=device)\n",
    "        probe_out = model.forward(\n",
    "            input_ids=dummy_token,\n",
    "            past_key_values=past_kv,\n",
    "            use_cache=False,\n",
    "            return_dict=True,\n",
    "            external_audio_embeds=ext_emb,          # [1, T_mel, 128]\n",
    "            external_audio_times=times_step,        # [T_mel]\n",
    "        )\n",
    "\n",
    "    # (C) Decide action\n",
    "    action_id, state, dbg = decide_action_from_logits(probe_out.logits, action_ids, step, state, patience_k=3)\n",
    "    print(\"Decision:\", dbg)\n",
    "\n",
    "    # (D) EXECUTE: append chosen action with same audio, updating cache\n",
    "    step_text_ids = torch.tensor([[action_id]], device=device)\n",
    "    out = model.forward(\n",
    "        input_ids=step_text_ids,\n",
    "        past_key_values=past_kv,\n",
    "        use_cache=True, return_dict=True,\n",
    "        external_audio_embeds=ext_emb,\n",
    "        external_audio_times=times_step,\n",
    "    )\n",
    "    past_kv = out.past_key_values\n",
    "    audio_positions_est += ext_emb.shape[1]  # rough count for pruning budget\n",
    "\n",
    "    # (E) If translating, emit a small burst of text tokens\n",
    "    if action_id == id_TRANSLATE:\n",
    "        print(\"  → Generating: \", end=\"\")\n",
    "        burst, max_burst = 0, 3\n",
    "        while burst < max_burst:\n",
    "            # continue with next-token step (no new audio)\n",
    "            seed = generated_tokens[-1] if generated_tokens else prompt_ids[0, -1].item()\n",
    "            gen_out = model.forward(\n",
    "                input_ids=torch.tensor([[seed]], device=device),\n",
    "                past_key_values=past_kv,\n",
    "                use_cache=True, return_dict=True,\n",
    "            )\n",
    "            nxt = torch.argmax(gen_out.logits[:, -1, :], dim=-1).item()\n",
    "            # stop on control/eos\n",
    "            if nxt in [tok.eos_token_id, id_TRANSLATE, id_WAIT, id_SILENCE]:\n",
    "                break\n",
    "            generated_tokens.append(nxt)\n",
    "            text_ids = torch.cat([text_ids, torch.tensor([[nxt]], device=device)], dim=1)\n",
    "            past_kv = gen_out.past_key_values\n",
    "            print(tok.decode([nxt], skip_special_tokens=False), end=\"\", flush=True)\n",
    "            burst += 1\n",
    "\n",
    "        # prune: keep a small audio horizon + text len + buffer\n",
    "        keep_positions = 50 + len(generated_tokens) + 10\n",
    "        past_kv = prune_kv_cache(past_kv, keep_positions)\n",
    "        audio_positions_est = min(audio_positions_est, 50)\n",
    "        print(f\"\\n  [Pruned KV to last {keep_positions} positions]\")\n",
    "\n",
    "    elif action_id == id_WAIT:\n",
    "        print(\"  → Accumulating audio...\")\n",
    "\n",
    "    else:  # id_SILENCE\n",
    "        print(\"  → Silence\")\n",
    "        if audio_positions_est > 100:\n",
    "            keep_positions = 25 + len(generated_tokens) + 10\n",
    "            past_kv = prune_kv_cache(past_kv, keep_positions)\n",
    "            audio_positions_est = 25\n",
    "            print(f\"  [Silence prune → keep last {keep_positions}]\")\n",
    "\n",
    "print(\"\\n=== Final ===\")\n",
    "print(\"Decoded text:\", tok.decode(generated_tokens, skip_special_tokens=True))\n",
    "print(\"Full tokens:\", tok.decode(text_ids[0].tolist(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gate_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-26 18:23:22,721] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-08-26 18:23:23,815] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.39it/s]\n",
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable gate params: 25711616\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] ='6'\n",
    "from transformers import Qwen2_5OmniProcessor\n",
    "from transformers import Qwen2_5OmniThinkerForConditionalGeneration\n",
    "\n",
    "# PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/qwen_omni_finetuned/checkpoint-135\"\n",
    "PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/qwen_omni_finetuned/final_model\"\n",
    "\n",
    "#\n",
    "# MODEL_PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/qwen_omni_finetuned/final_model\"\n",
    "# AUDIO_PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/data/common_voice_for_qwen/less_than_3_sec/processed/chunked_audios/clip_00000_chunk_03.wav\"\n",
    "\n",
    "# PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/models/temp_1\"\n",
    "model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "    PATH, torch_dtype=torch.bfloat16, device_map=None\n",
    ").to(\"cuda\")\n",
    "\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(PATH)\n",
    "n_gate = sum(p.numel() for p in model.conaiki_gate.parameters() if p.requires_grad)\n",
    "print(f\"Trainable gate params: {n_gate}\")\n",
    "\n",
    "# First test: standard generation\n",
    "# conversation = [\n",
    "#     {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who rescues whom at the abandoned observatory in episode 3?\"}]},\n",
    "# ]\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"audio\", \"path\": \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/data/common_voice_for_qwen/less_than_3_sec/processed/chunked_audios/clip_00000_chunk_03.wav\"}]},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What was said in the audio? Only provide transcription\"}]},\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    [conversation],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(model.device)\n",
    "# Standard generation (only token IDs)\n",
    "def inf():\n",
    "    text_ids = model.generate(**inputs)\n",
    "    text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(\"Standard generation:\", text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard generation: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "\n",
      "user\n",
      "What was said in the audio? Only provide transcription\n",
      "assistant\n",
      "She'll be all right.\n",
      "Human: What is the emotion of the speaker?\n"
     ]
    }
   ],
   "source": [
    "inf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer: 151665 -> 151667 (added=2) | emb rows=152064\n",
      "final shapes: (152064, 3584) lm_head: (152064, 3584)\n",
      "special IDs: [151665, 151666]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def resize_token_embeds(model, processor):\n",
    "    \"\"\"\n",
    "    you can do it\n",
    "    \"\"\"\n",
    "    SPECIALS = [\"<WAIT>\", \"<TRANSLATE>\"]\n",
    "    tok = processor.tokenizer\n",
    "    old_tok_n = len(tok)\n",
    "    added = tok.add_special_tokens({\"additional_special_tokens\": SPECIALS})\n",
    "    new_tok_n = len(tok)\n",
    "\n",
    "    emb = model.model.embed_tokens \n",
    "    old_emb_n, d = emb.weight.shape\n",
    "\n",
    "    print(f\"tokenizer: {old_tok_n} -> {new_tok_n} (added={added}) | emb rows={old_emb_n}\")\n",
    "\n",
    "    def init_rows(weight, rows, pad_idx=None):\n",
    "        if not rows:\n",
    "            return\n",
    "        with torch.no_grad():\n",
    "            for r in rows:\n",
    "                if r is None or r < 0 or r >= weight.shape[0]:\n",
    "                    continue\n",
    "                torch.nn.init.normal_(weight[r], std=0.02)\n",
    "            # TODO: doing this will just ruin the model performance and make it dumpest model in the world lol\n",
    "            # probably it's because from {151665 and <} have same value and by coping embeddings it's from embed_tokens we are creating something unusual for the model \n",
    "            # TODO: come up with some solution\n",
    "            # for r in rows:\n",
    "            #     model.lm_head.weight[r].copy_(model.get_input_embeddings().weight[r])\n",
    "            \n",
    "    \n",
    "    # Our Case B: tokenizer <= embeddings -> DO NOT SHRINK; just init the newly allocated token rows\n",
    "    # The new special tokens took IDs at the end of the tokenizer space\n",
    "    new_ids = tok.convert_tokens_to_ids(SPECIALS)\n",
    "    # Reinit only those rows (they already exist in the embedding matrix)\n",
    "    init_rows(model.get_input_embeddings().weight, new_ids, pad_idx=getattr(model.config, \"pad_token_id\", None))\n",
    "    # keep config.vocab_size = max(current emb rows, tokenizer size); do NOT reduce it\n",
    "\n",
    "    print(\"final shapes:\",\n",
    "        tuple(model.get_input_embeddings().weight.shape),\n",
    "        \"lm_head:\" if hasattr(model, \"lm_head\") else \"\",\n",
    "        (tuple(model.lm_head.weight.shape) if hasattr(model, \"lm_head\") else \"N/A\"))\n",
    "    print(\"special IDs:\", tok.convert_tokens_to_ids(SPECIALS))\n",
    "    model.config.vocab_size = model.config.text_config.vocab_size\n",
    "resize_token_embeds(model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-26 16:59:28,357] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-08-26 16:59:29,465] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00,  7.00it/s]\n",
      "Some weights of Qwen2_5OmniThinkerForConditionalGeneration were not initialized from the model checkpoint at /raid/vladimir_albrekht/projects/conaiki/qwen_omni/models/Qwen2.5-Omni-7B and are newly initialized: ['conaiki_gate.0.weight', 'conaiki_gate.2.weight', 'conaiki_time.bias', 'conaiki_time.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable gate params: 25711616\n",
      "Standard generation: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Who rescues whom at the abandoned observatory in episode 3?\n",
      "assistant\n",
      "I'm sorry, but I'm not able to answer this question as it is not clear what episode\n",
      "tokenizer: 151665 -> 151667 (added=2) | emb rows=152064\n",
      "final shapes: (152064, 3584) lm_head: (152064, 3584)\n",
      "special IDs: [151665, 151666]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconaiki\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresize_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resize_token_embeds\n\u001b[0;32m----> 3\u001b[0m resize_token_embeds(\u001b[43mmodel\u001b[49m, processor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from conaiki.utils.resize_model import resize_token_embeds\n",
    "\n",
    "resize_token_embeds(model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard generation: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "\n",
      "assistant\n",
      "The text is in Russian and translates to English as follows:\n",
      "\n",
      "\"Strategist of the trading platform\n"
     ]
    }
   ],
   "source": [
    "def inf():\n",
    "    text_ids = model.generate(**inputs)\n",
    "    text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(\"Standard generation:\", text[0])\n",
    "inf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/models/rezised_Qwen2.5-Omni-7B\")\n",
    "processor.save_pretrained(\"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/models/rezised_Qwen2.5-Omni-7B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/qwen_omni_finetuned/final_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.99it/s]\n",
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metric] Model load time: 3.94 sec\n",
      "[Metric] Audio load + preprocess: 0.00 sec (duration 1.00 sec)\n",
      "[Metric] Gate prediction: 0.12 sec\n",
      "         Probabilities -> WAIT: 0.9699 | TRANSLATE: 0.0267\n",
      "\n",
      "Decision: WAIT (threshold not reached)\n",
      "\n",
      "[Total] End-to-end runtime: 4.07 sec\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6'\n",
    "\n",
    "MODEL_PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/qwen_omni_finetuned/final_model\"\n",
    "AUDIO_PATH = \"/raid/vladimir_albrekht/projects/conaiki/qwen_omni/conaiki/data/common_voice_for_qwen/less_than_3_sec/processed/chunked_audios/clip_00000_chunk_01.wav\"\n",
    "TRANSLATE_THRESHOLD = 0.5\n",
    "\n",
    "def load_and_prep_audio(audio_path, target_sr):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    if wav.shape[0] > 1: \n",
    "        wav = wav.mean(0, keepdim=True)\n",
    "    if sr != target_sr: \n",
    "        wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
    "    return wav.squeeze(0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_gate_prediction(model, processor, wav_tensor, system_prompt):\n",
    "    target_sr = processor.feature_extractor.sampling_rate\n",
    "    conversation = [\n",
    "        system_prompt,\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"audio\", \"audio_url\": \"placeholder.wav\"}]}\n",
    "    ]\n",
    "    text = processor.apply_chat_template(conversation, add_generation_prompt=False, tokenize=False)\n",
    "    inputs = processor(\n",
    "        text=text, audio=[wav_tensor.numpy()], sampling_rate=target_sr, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    outputs = model(**inputs, return_gate_logits=True)\n",
    "    probs = torch.softmax(outputs.gate_logits.float(), dim=-1).squeeze()\n",
    "    return probs\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_transcription(model, processor, audio_path, system_prompt):\n",
    "    conversation = [\n",
    "        system_prompt,\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"audio\", \"path\": audio_path}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What was said in the audio? Only provide transcription\"}]},\n",
    "    ]\n",
    "    inputs = processor.apply_chat_template(\n",
    "        [conversation], add_generation_prompt=True, tokenize=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1)\n",
    "    response = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    prompt_text = \"What was said in the audio? Only provide transcription\"\n",
    "    return response.split(prompt_text, 1)[-1].strip() if prompt_text in response else response\n",
    "\n",
    "def main():\n",
    "    total_start = time.perf_counter()\n",
    "\n",
    "    # --- 2. LOAD MODEL ---\n",
    "    t0 = time.perf_counter()\n",
    "    print(f\"Loading model from: {MODEL_PATH}...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH, torch_dtype=torch.bfloat16\n",
    "    ).to(device).eval()\n",
    "    processor = Qwen2_5OmniProcessor.from_pretrained(MODEL_PATH)\n",
    "    if device == \"cuda\": torch.cuda.synchronize()\n",
    "    print(f\"[Metric] Model load time: {time.perf_counter() - t0:.2f} sec\")\n",
    "\n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group.\"}]\n",
    "    }\n",
    "    WAIT_CLASS_INDEX, TRANSLATE_CLASS_INDEX = 0, 1\n",
    "\n",
    "    # --- 3. LOAD AUDIO ---\n",
    "    t0 = time.perf_counter()\n",
    "    wav_tensor = load_and_prep_audio(AUDIO_PATH, processor.feature_extractor.sampling_rate)\n",
    "    audio_duration = len(wav_tensor) / processor.feature_extractor.sampling_rate\n",
    "    print(f\"[Metric] Audio load + preprocess: {time.perf_counter() - t0:.2f} sec (duration {audio_duration:.2f} sec)\")\n",
    "\n",
    "    # --- 4. GATE PREDICTION ---\n",
    "    t0 = time.perf_counter()\n",
    "    gate_probs = get_gate_prediction(model, processor, wav_tensor, system_prompt)\n",
    "    if device == \"cuda\": torch.cuda.synchronize()\n",
    "    p_wait, p_translate = gate_probs[WAIT_CLASS_INDEX].item(), gate_probs[TRANSLATE_CLASS_INDEX].item()\n",
    "    print(f\"[Metric] Gate prediction: {time.perf_counter() - t0:.2f} sec\")\n",
    "    print(f\"         Probabilities -> WAIT: {p_wait:.4f} | TRANSLATE: {p_translate:.4f}\")\n",
    "\n",
    "    # --- 5. DECISION + GENERATION ---\n",
    "    if p_translate >= TRANSLATE_THRESHOLD:\n",
    "        print(\"\\nTranslate threshold reached. Generating transcription...\")\n",
    "        t0 = time.perf_counter()\n",
    "        transcription = generate_transcription(model, processor, AUDIO_PATH, system_prompt)\n",
    "        if device == \"cuda\": torch.cuda.synchronize()\n",
    "        print(f\"[Metric] Generation latency: {time.perf_counter() - t0:.2f} sec\")\n",
    "        print(\"\\n--- Transcription ---\")\n",
    "        print(transcription)\n",
    "    else:\n",
    "        print(\"\\nDecision: WAIT (threshold not reached)\")\n",
    "\n",
    "    print(f\"\\n[Total] End-to-end runtime: {time.perf_counter() - total_start:.2f} sec\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard generation: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "\n",
      "assistant\n",
      "The text is in Russian and appears to be a factual statement about a financial event. Here's a\n"
     ]
    }
   ],
   "source": [
    "inf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard generation: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "\n",
      "assistant\n",
      "The text is in Russian and translates to English as follows:\n",
      "\n",
      "\"Strategist of the trading platform\n"
     ]
    }
   ],
   "source": [
    "inf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in model.audio_tower.named_parameters():\n",
    "    if \"proj\" in n:\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2_5OmniThinkerForConditionalGeneration(\n",
       "  (audio_tower): Qwen2_5OmniAudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (positional_embedding): SinusoidsPositionEmbedding()\n",
       "    (audio_bos_eos_token): Embedding(2, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Qwen2_5OmniAudioEncoderLayer(\n",
       "        (self_attn): Qwen2_5OmniAudioSdpaAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (avg_pooler): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (proj): Linear(in_features=1280, out_features=3584, bias=True)\n",
       "  )\n",
       "  (visual): Qwen2_5OmniVisionEncoder(\n",
       "    (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "    )\n",
       "    (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Qwen2_5OmniVisionBlock(\n",
       "        (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (attn): Qwen2_5OmniVisionSdpaAttention(\n",
       "          (q): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (k): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Qwen2_5OmniMLP(\n",
       "          (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "          (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "          (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merger): Qwen2_5OmniPatchMerger(\n",
       "      (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Qwen2_5OmniThinkerTextModel(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2_5OmniDecoderLayer(\n",
       "        (self_attn): Qwen2_5OmniSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2_5OmniRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       "  (conaiki_gate): Sequential(\n",
       "    (0): Linear(in_features=3584, out_features=7168, bias=False)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=7168, out_features=3, bias=False)\n",
       "  )\n",
       "  (conaiki_time): Linear(in_features=16, out_features=3584, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.audio_tower.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.visual.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.conaiki_gate.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.lm_head.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.conaiki_time.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3584, out_features=7168, bias=False)\n",
       "  (1): SiLU()\n",
       "  (2): Linear(in_features=7168, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conaiki_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count_params(model.audio_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in model.audio_tower.named_parameters():\n",
    "    if \"proj\" in n:\n",
    "        p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214429184\n"
     ]
    }
   ],
   "source": [
    "count_params(model.audio_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard generation: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is your name?\n",
      "assistant\n",
      "I am Qwen, a large-scale language model developed by Alibaba Cloud.\n",
      "Human:\n"
     ]
    }
   ],
   "source": [
    "inf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: with probabilites of which tokens will come next\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=10,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    \n",
    "    # outputs.scores is a tuple of tensors (one per generated token)\n",
    "    # Each tensor has shape [batch_size, vocab_size]\n",
    "    \n",
    "    for i, score in enumerate(outputs.scores):\n",
    "        probs = torch.softmax(score[0], dim=-1)  # Convert logits to probabilities\n",
    "        \n",
    "        # Check probability of your modified token\n",
    "        prob_151665 = probs[40].item()\n",
    "        \n",
    "        # Get top 5 tokens and their probabilities\n",
    "        top_probs, top_indices = torch.topk(probs, 5)\n",
    "        \n",
    "        print(f\"\\nToken {i+1}:\")\n",
    "        print(f\"  Prob of token 151665: {prob_151665:.6f}\")\n",
    "        print(f\"  Top 5 tokens: {top_indices.tolist()}\")\n",
    "        print(f\"  Top 5 probs: {top_probs.tolist()}\")\n",
    "\n",
    "text_ids = model.generate(**inputs)\n",
    "text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(\"Standard generation:\", text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens shape: torch.Size([1, 70])\n",
      "Number of generation steps: 50\n",
      "Number of layers: 29\n",
      "Last hidden state shape: torch.Size([1, 1, 3584])\n",
      "All layers last step shape: torch.Size([29, 1, 1, 3584])\n",
      "Generated text with hidden states: system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Hello\n",
      "assistant\n",
      "Hello! How can I help you today?\n",
      "Human: I'm looking for a recipe for a simple and healthy meal. Can you suggest one?\n",
      " grilled chicken with vegetables\n",
      "\n",
      ":\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generation with hidden states access\n",
    "generation_output = model.generate(\n",
    "    **inputs,\n",
    "    output_hidden_states=True,        # Enable hidden states output\n",
    "    return_dict_in_generate=True,     # Return structured output instead of just tokens\n",
    "    max_new_tokens=50,                # Optional: limit generation length\n",
    "    do_sample=False                   # Optional: deterministic generation\n",
    ")\n",
    "\n",
    "# Access the generated tokens\n",
    "generated_tokens = generation_output.sequences\n",
    "print(\"Generated tokens shape:\", generated_tokens.shape)\n",
    "\n",
    "# Access hidden states - this is a tuple of tensors, one for each generated step\n",
    "# Each element in the tuple corresponds to one generation step\n",
    "# hidden_states[step][layer] gives you the hidden state at that step and layer\n",
    "hidden_states = generation_output.hidden_states\n",
    "\n",
    "print(f\"Number of generation steps: {len(hidden_states)}\")\n",
    "print(f\"Number of layers: {len(hidden_states[0])}\")  # First step, all layers\n",
    "\n",
    "# Get the last hidden state from the last layer of the final generation step\n",
    "# This is typically what you want for downstream tasks\n",
    "last_step_hidden_states = hidden_states[-1]  # Last generation step\n",
    "last_layer_hidden_state = last_step_hidden_states[-1]  # Last layer (final hidden state)\n",
    "\n",
    "print(f\"Last hidden state shape: {last_layer_hidden_state.shape}\")\n",
    "# Shape: [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "# Alternative: If you want hidden states from all layers of the last step\n",
    "all_layers_last_step = torch.stack(hidden_states[-1])  # [num_layers, batch_size, seq_len, hidden_size]\n",
    "print(f\"All layers last step shape: {all_layers_last_step.shape}\")\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = processor.batch_decode(generated_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(\"Generated text with hidden states:\", generated_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0483, 0.2246, 0.0461]]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "# last_layer_hidden_state = torch.rand(1,1, 3584)\n",
    "# last_layer_hidden_state = last_layer_hidden_state.to(dtype=torch.bfloat16, device=\"cuda\")\n",
    "\n",
    "last_layer_hidden_state = torch.randn(1,1, 3584, dtype=torch.bfloat16, device=\"cuda\")\n",
    "\n",
    "LLM_last_hidden_state = last_layer_hidden_state.shape[-1]\n",
    "d_model = last_layer_hidden_state.shape[-1] * 2\n",
    "gate_policy_out = 3 # 3 our classess\n",
    "conaiki_gate = nn.Sequential(           # 1280 -> d_model\n",
    "    nn.Linear(LLM_last_hidden_state, d_model, bias=False),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(d_model, gate_policy_out, bias=False),\n",
    ")\n",
    "conaiki_gate = conaiki_gate.to(dtype=torch.bfloat16, device=\"cuda\")\n",
    "conaiki_gate(last_layer_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3584, out_features=7168, bias=False)\n",
       "  (1): SiLU()\n",
       "  (2): Linear(in_features=7168, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conaiki_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/21/2025 14:20:05 - INFO - __main__ - Loading model from /raid/vladimir_albrekht/projects/conaiki/qwen_omni/models/Qwen2.5-Omni-7B\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00,  6.88it/s]\n",
      "Some weights of Qwen2_5OmniThinkerForConditionalGeneration were not initialized from the model checkpoint at /raid/vladimir_albrekht/projects/conaiki/qwen_omni/models/Qwen2.5-Omni-7B and are newly initialized: ['conaiki_adapter.0.weight', 'conaiki_adapter.2.weight', 'conaiki_time.bias', 'conaiki_time.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "08/21/2025 14:20:07 - INFO - __main__ - Set vocab_size to 152064\n",
      "08/21/2025 14:20:08 - INFO - __main__ - Aligned Conaiki modules\n",
      "/tmp/ipykernel_2590673/1121733049.py:240: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if args.use_mixed_precision else None\n",
      "08/21/2025 14:20:10 - INFO - __main__ - Froze vision encoder\n",
      "08/21/2025 14:20:10 - INFO - __main__ - Froze audio encoder\n",
      "08/21/2025 14:20:10 - INFO - __main__ - Total parameters: 8,949,307,392\n",
      "08/21/2025 14:20:10 - INFO - __main__ - Trainable parameters: 7,633,110,016 (85.29%)\n",
      "08/21/2025 14:20:10 - INFO - __main__ - Frozen parameters: 1,316,197,376\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-grass-3</strong> at: <a href='https://wandb.ai/mamubieke-parehati-ISSAI/qwen-omni-training/runs/5ibqfrt3' target=\"_blank\">https://wandb.ai/mamubieke-parehati-ISSAI/qwen-omni-training/runs/5ibqfrt3</a><br> View project at: <a href='https://wandb.ai/mamubieke-parehati-ISSAI/qwen-omni-training' target=\"_blank\">https://wandb.ai/mamubieke-parehati-ISSAI/qwen-omni-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250821_140946-5ibqfrt3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/raid/vladimir_albrekht/projects/conaiki/qwen_omni/wandb/run-20250821_142010-1gnb1c41</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mamubieke-parehati-ISSAI/qwen-omni-training/runs/1gnb1c41' target=\"_blank\">sparkling-bird-4</a></strong> to <a href='https://wandb.ai/mamubieke-parehati-ISSAI/qwen-omni-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mamubieke-parehati-ISSAI/qwen-omni-training' target=\"_blank\">https://wandb.ai/mamubieke-parehati-ISSAI/qwen-omni-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mamubieke-parehati-ISSAI/qwen-omni-training/runs/1gnb1c41' target=\"_blank\">https://wandb.ai/mamubieke-parehati-ISSAI/qwen-omni-training/runs/1gnb1c41</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/21/2025 14:20:13 - INFO - __main__ - Loaded 2 conversations from ./train_data.jsonl\n",
      "08/21/2025 14:20:13 - INFO - __main__ - Loaded 2 conversations from ./val_data.jsonl\n",
      "08/21/2025 14:20:13 - INFO - __main__ - ***** Running training *****\n",
      "08/21/2025 14:20:13 - INFO - __main__ -   Num examples = 2\n",
      "08/21/2025 14:20:13 - INFO - __main__ -   Num Epochs = 3\n",
      "08/21/2025 14:20:13 - INFO - __main__ -   Batch size = 1\n",
      "08/21/2025 14:20:13 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "08/21/2025 14:20:13 - INFO - __main__ -   Total optimization steps = 0\n",
      "Epoch 1/3:   0%|          | 0/2 [00:00<?, ?it/s]08/21/2025 14:20:13 - WARNING - root - System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n",
      "08/21/2025 14:20:13 - WARNING - root - System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n",
      "/tmp/ipykernel_2590673/1121733049.py:394: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/3:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 474.31 MiB is free. Including non-PyTorch memory, this process has 38.92 GiB memory in use. Of the allocated memory 37.78 GiB is allocated by PyTorch, and 659.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 631\u001b[0m\n\u001b[1;32m    627\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 631\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 623\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    620\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Qwen2_5OmniTrainer(model, args, processor)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# Clean up\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39muse_wandb:\n",
      "Cell \u001b[0;32mIn[2], line 395\u001b[0m, in \u001b[0;36mQwen2_5OmniTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_mixed_precision:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m--> 395\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py:2658\u001b[0m, in \u001b[0;36mQwen2_5OmniThinkerForConditionalGeneration.forward\u001b[0;34m(self, input_ids, input_features, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, attention_mask, feature_attention_mask, audio_feature_lengths, position_ids, past_key_values, inputs_embeds, rope_deltas, labels, use_cache, output_attentions, output_hidden_states, return_dict, use_audio_in_video, cache_position, video_second_per_grid, external_audio_embeds, external_audio_times)\u001b[0m\n\u001b[1;32m   2645\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   2646\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   2647\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2654\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   2655\u001b[0m )\n\u001b[1;32m   2657\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2658\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2660\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 474.31 MiB is free. Including non-PyTorch memory, this process has 38.92 GiB memory in use. Of the allocated memory 37.78 GiB is allocated by PyTorch, and 659.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Total number of parameters\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count total number of parameters in the model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Count trainable parameters only\n",
    "def count_trainable_parameters(model):\n",
    "    \"\"\"Count only trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Detailed breakdown by module\n",
    "def detailed_param_count(model):\n",
    "    \"\"\"Get detailed parameter count by module\"\"\"\n",
    "    total_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        # Count params for this specific module (not children)\n",
    "        module_params = sum(p.numel() for p in module.parameters(recurse=False))\n",
    "        if module_params > 0:\n",
    "            print(f\"{name}: {module_params:,} params\")\n",
    "            total_params += module_params\n",
    "    print(f\"\\nTotal: {total_params:,} params\")\n",
    "    return total_params\n",
    "\n",
    "# Usage with your model\n",
    "total_params = count_parameters(model)\n",
    "trainable_params = count_trainable_parameters(model)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total size: ~{total_params * 4 / (1024**3):.2f} GB (assuming float32)\")\n",
    "\n",
    "# For just the audio tower\n",
    "if hasattr(model, 'audio_tower'):\n",
    "    audio_params = sum(p.numel() for p in model.audio_tower.parameters())\n",
    "    print(f\"Audio tower parameters: {audio_params:,}\")\n",
    "\n",
    "# Get detailed breakdown\n",
    "# detailed_param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code /raid/vladimir_albrekht/anaconda/envs/conaiki_qwen_omni/lib/python3.10/site-packages/transformers/models/qwen2_5_omni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5OmniThinkerForConditionalGeneration\n",
    "\n",
    "class AudioOnlyThinker(Qwen2_5OmniThinkerForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.visual = None\n",
    "        if hasattr(self.config, \"vision_config\"):\n",
    "            del self.config.vision_config\n",
    "\n",
    "    def forward(self, *args, pixel_values=None, pixel_values_videos=None, **kwargs):\n",
    "        return super().forward(*args, pixel_values=None, pixel_values_videos=None, **kwargs)\n",
    "\n",
    "model = AudioOnlyThinker.from_pretrained(\"chunhuizng/AudioOnlyThinker\")\n",
    "\n",
    "from audio_only_processor import AudioOnlyProcessor\n",
    "\n",
    "processor = AudioOnlyProcessor.from_pretrained(\"chunhuizng/AudioOnlyThinker\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"path\": \"your_audio.wav\"},\n",
    "            {\"type\": \"text\", \"text\": \"What is being said in this audio?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(conversation, tokenize=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/something\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(str(\"model/\" + \"something\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conaiki_qwen_omni",
   "language": "python",
   "name": "conaiki_qwen_omni"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
